{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Classification Using Scikit-learn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\r\n",
    "\r\n",
    "*    [ A Kubernetes cluster deployed on Azure Stack HCI, connected to Azure through ARC](https://docs.microsoft.com/en-us/azure-stack/aks-hci/connect-to-arc).\r\n",
    "     \r\n",
    "\r\n",
    "*    [ Datastore setup in Azure Machine Learning workspace backed up by Azure Stack Hub storage account ](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md) \r\n",
    "\r\n",
    "\r\n",
    "*    Last but not least, you need to be able to run a Notebook. (azureml-core, numpy, matplotlib, requests are required)\r\n",
    "\r\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\r\n",
    "from azureml.exceptions import ComputeTargetException\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download mnist data\n",
    "\n",
    "Perform pip install azureml-opendatasets to get the open dataset package, use this function to download mnist data later. This allows you to avoid download the data again when you run this notebook multiple times. The actual download time may take 2 minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Dataset\r\n",
    "from azureml.opendatasets import MNIST\r\n",
    "import os\r\n",
    "\r\n",
    "def download_mnist_data():\r\n",
    "    data_folder = os.path.join(os.getcwd(), 'mnist_data')\r\n",
    "    os.makedirs(data_folder, exist_ok=True)\r\n",
    "\r\n",
    "    mnist_file_dataset = MNIST.get_file_dataset()\r\n",
    "    path = mnist_file_dataset.download(data_folder, overwrite=True)\r\n",
    "    downloaded_folder = os.path.dirname(path[0])\r\n",
    "    print(\"downloaded to\", downloaded_folder)\r\n",
    "    \r\n",
    "    return downloaded_folder\r\n",
    "\r\n",
    "download_mnist_data()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "The above download_mnist_data() function will download four files  t10k-images-idx3-ubyte.gz, t10k-labels-idx1-ubyte.gz, train-images-idx3-ubyte.gz and train-labels-idx1-ubyte.gz to downloaded_folder.  Your next step is to upload these files to datastore of the workspace, and then registered as dataset in the workspace. \n",
    "\n",
    "\"datastore_name\" is the name of the datastore you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/Train-AzureArc.md).\n",
    "\n",
    "Upload and dataset registration take less than 1 min."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore\r\n",
    "\r\n",
    "dataset_name = \"mnist_ash_o\"\r\n",
    "datastore_name = \"ashdatastore\"\r\n",
    "\r\n",
    "if dataset_name not  in ws.datasets:\r\n",
    "    downloaded_folder = download_mnist_data()\r\n",
    "    datastore =  Datastore.get(ws, datastore_name)\r\n",
    "    \r\n",
    "    src_dir, target_path =downloaded_folder, 'mnistdataash'\r\n",
    "    datastore.upload(src_dir, target_path)\r\n",
    "\r\n",
    "    # register data uploaded as AML dataset\r\n",
    "    datastore_paths = [(datastore, target_path)]\r\n",
    "    mnist_ds = Dataset.File.from_files(path=datastore_paths)\r\n",
    "    mnist_ds.register(ws, dataset_name, \"mnist data from http://yann.lecun.com/exdb/mnist/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup compute target\r\n",
    "\r\n",
    "Find the Arc K8S Resource Id, e.g. /subscriptions/86204643-5a96-427b-b6bb-b35b2bd6e6ce/resourceGroups/AKS-HCI2/providers/Microsoft.Kubernetes/connectedClusters/my-workload-cluster and replace the resource id below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import KubernetesCompute\r\n",
    "from azureml.core.compute import ComputeTarget\r\n",
    "import os\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "\r\n",
    "# choose a name for your Azure Arc-enabled Kubernetes compute\r\n",
    "amlarc_compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"amlarc-compute4\")\r\n",
    "\r\n",
    "# resource ID for your Azure Arc-enabled Kubernetes cluster\r\n",
    "resource_id = \"/subscriptions/86204643-5a96-427b-b6bb-b35b2bd6e6ce/resourceGroups/AKS-HCI2/providers/Microsoft.Kubernetes/connectedClusters/my-workload-cluster\"\r\n",
    "\r\n",
    "if amlarc_compute_name in ws.compute_targets:\r\n",
    "   amlarc_compute = ws.compute_targets[amlarc_compute_name]\r\n",
    "   if amlarc_compute and type(amlarc_compute) is KubernetesCompute:\r\n",
    "      print(\"found compute target: \" + amlarc_compute_name)\r\n",
    "else:\r\n",
    "   print(\"creating new compute target...\")\r\n",
    "   ns = \"aml\"\r\n",
    "    \r\n",
    "   instance_types = {\r\n",
    "    \"defaultInstanceType\": {\r\n",
    "      \"nodeSelector\": None,\r\n",
    "      \"resources\": {\r\n",
    "        \"requests\": {\r\n",
    "          \"cpu\": \"1\",\r\n",
    "          \"memory\": \"4Gi\",\r\n",
    "          \"nvidia.com/gpu\": 0\r\n",
    "        },\r\n",
    "        \"limits\": {\r\n",
    "          \"cpu\": \"1\",\r\n",
    "          \"memory\": \"4Gi\",\r\n",
    "          \"nvidia.com/gpu\": 0\r\n",
    "        }\r\n",
    "      }\r\n",
    "    }\r\n",
    "  }\r\n",
    "\r\n",
    "   amlarc_attach_configuration = KubernetesCompute.attach_configuration(resource_id = resource_id, namespace = ns, default_instance_type=\"defaultInstanceType\", instance_types = instance_types)\r\n",
    " \r\n",
    "   amlarc_compute = ComputeTarget.attach(ws, amlarc_compute_name, amlarc_attach_configuration)\r\n",
    "\r\n",
    " \r\n",
    "   amlarc_compute.wait_for_completion(show_output=True)\r\n",
    "    \r\n",
    "   # For a more detailed view of current KubernetesCompute status, use get_status()\r\n",
    "   print(amlarc_compute.get_status().serialize())\r\n",
    "\r\n",
    "print(f\"compute target id in endpoint yaml: azureml:{amlarc_compute.name}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.compute import KubernetesCompute\r\n",
    "\r\n",
    "attach_name = amlarc_compute_name\r\n",
    "arcK_target = KubernetesCompute(ws, attach_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure the training job and submit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create an experiement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Experiment\r\n",
    "\r\n",
    "experiment_name = 'mnist-demo'\r\n",
    "\r\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create an environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use a curated environment that has already been built for you\r\n",
    "\r\n",
    "from azureml.core.environment import Environment\r\n",
    "env = Environment.get(workspace=ws, \r\n",
    "                      name=\"AzureML-Scikit-learn0.24-Cuda11-OpenMpi4.1.0-py36\", \r\n",
    "                      version=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# customized environment\r\n",
    "\r\n",
    "# from azureml.core.environment import Environment\r\n",
    "# from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "# # to install required packages\r\n",
    "# env = Environment('tutorial-env')\r\n",
    "# cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]', 'azureml-defaults'], conda_packages = ['scikit-learn==0.22.1'])\r\n",
    "\r\n",
    "# env.python.conda_dependencies = cd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Configure the training job\n",
    "\n",
    "The training takes about 15 mins with vm size comparable  to Standard_DS3_v2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import ScriptRunConfig\r\n",
    "\r\n",
    "args = ['--data-folder', ws.datasets[dataset_name].as_mount(), '--regularization', 0.5]\r\n",
    "script_folder =  \"mnist_script\"\r\n",
    "src = ScriptRunConfig(source_directory=script_folder,\r\n",
    "                      script='train.py', \r\n",
    "                      arguments=args,\r\n",
    "                      compute_target=arcK_target,\r\n",
    "                      environment=env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Submit the job\n",
    "\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run = exp.submit(config=src)\r\n",
    "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name='sklearn_mnist'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# register model\r\n",
    "model = run.register_model(model_name=model_name,\r\n",
    "                           model_path='outputs/sklearn_mnist_model.pkl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The machine learning model named \"sklearn_mnist\" should be registered in your AzureML workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core.model import Model\r\n",
    "model = Model(ws, model_name)\r\n",
    "model_id = f\"azureml:{model.name}:{model.version}\"\r\n",
    "print(f\"Get {model.name}, latest version {model.version}, id in endpoint.yml: {model_id}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy and score a machine learning model by using a managed online endpoint\r\n",
    "\r\n",
    "AZ CLI only now"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "endpoint = 'sklearn-mnist-jiadu'\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "prefix = Path(__file__).parent\r\n",
    "endpoint_file = str(prefix.joinpath(\"endpoint.yml\"))\r\n",
    "print(f\"Using Endpoint file: {endpoint_file}, please replace model id (e.g. azureml:sklearn_mnist:2) and compute target id (e.g. azureml:amlarc-compute4) according above output\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import helpers\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "print('Workspace name: ' + ws.name, \r\n",
    "      'Azure region: ' + ws.location, \r\n",
    "      'Subscription id: ' + ws.subscription_id, \r\n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\r\n",
    "helpers.run(f\"az ml endpoint create -n {endpoint} -f {endpoint_file} -w {ws.name} -g {ws.resource_group}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test training model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test with inputs\r\n",
    "\r\n",
    "Here you may use the image from test asset. The first 30 images and its labels are shown below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mnist_script.utils import load_data\r\n",
    "import os\r\n",
    "import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "data_folder = os.path.join(os.getcwd(), 'mnist_data')\r\n",
    "\r\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\r\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\r\n",
    "\r\n",
    "# show first 30 figures\r\n",
    "\r\n",
    "count = 0\r\n",
    "sample_size = 30\r\n",
    "plt.figure(figsize = (16, 6))\r\n",
    "# for i in np.random.permutation(X_test.shape[0])[:sample_size]:\r\n",
    "for i in range(30):\r\n",
    "    count = count + 1\r\n",
    "    plt.subplot(1, sample_size, count)\r\n",
    "    plt.axhline('')\r\n",
    "    plt.axvline('')\r\n",
    "    plt.text(x = 10, y = -10, s = y_test[i], fontsize = 18)\r\n",
    "    plt.imshow(X_test[i].reshape(28, 28), cmap = plt.cm.Greys)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get score_uri and access_token from AZ CLI (Currently only AZ CLI supported)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get predicted digits:\r\n",
    "import helpers\r\n",
    "from azureml.core.workspace import Workspace\r\n",
    "ws = Workspace.from_config()\r\n",
    "cmd = f\"az ml endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\r\n",
    "properties = helpers.run(cmd, return_output=True, no_output=True)\r\n",
    "\r\n",
    "cmd = f\"az ml endpoint get-credentials -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\r\n",
    "credentials = helpers.run(cmd, return_output=True, no_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the second image: 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "prop_response = json.loads(properties.replace(os.linesep,\"\"))\r\n",
    "score_uri = prop_response[\"scoring_uri\"]\r\n",
    "\r\n",
    "cred_response = json.loads(credentials.replace(os.linesep, \"\"))\r\n",
    "access_token = cred_response[\"accessToken\"]\r\n",
    "\r\n",
    "import requests\r\n",
    "# second number should be 2\r\n",
    "test = json.dumps({\"data\": X_test.tolist()[1:2]})\r\n",
    "headers = {'Content-Type': 'application/json', 'Authorization': f\"Bearer {access_token}\"}\r\n",
    "r = requests.post(score_uri, data=test, headers=headers)\r\n",
    "print(f\"predictions: {r.content}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next steps\n",
    "\n",
    "1. Learn how to [distributed training with pytorch](../distributed-cifar10/distributed-pytorch-cifar10.ipynb)\n",
    "2. Learn how to [distributed training with tensorflow](../distributed-cifar10/distributed-tf2-cifar10.ipynb)\n",
    "3. Learn Pipeline Steps with [Object Segmentation](../object-segmentation-on-azure-stack/)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "fc402497f0168b24575e2ffafe64cd34c507b9a7fab971a93b09782ae565c5c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}