{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Image Classification with PyTorch\n",
    "\n",
    "In this tutorial, you will train a PyTorch model on the [CIFAR10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset using distributed training with PyTorch's `DistributedDataParallel` module across an AKS-HCI CPU cluster. The training dataset are stored in NFS server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "*    [ A Kubernetes cluster deployed on Azure Stack HCI, connected to Azure through ARC](https://docs.microsoft.com/en-us/azure-stack/aks-hci/connect-to-arc).\n",
    "     \n",
    "\n",
    "*    [ Setup a NFS Server ](https://github.com/Azure/AML-Kubernetes/blob/master/docs/setup-ephemeral-nfs-volume.md) \n",
    "\n",
    "\n",
    "*    Last but not least, you need to be able to run a Notebook. (azureml-core, azureml-opendatasets, numpy, matplotlib, requests are required)\n",
    "\n",
    "\n",
    "   If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, make sure you go through the configuration Notebook located at [here](https://github.com/Azure/MachineLearningNotebooks) first. This sets you up with a working config file that has information on your workspace, subscription id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.34.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AzureML workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`. \n",
    "\n",
    "If you haven't done already please go to `config.json` file and fill in your workspace information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: demo-ws\n",
      "Azure region: eastus\n",
      "Subscription id: 86204643-5a96-427b-b6bb-b35b2bd6e6ce\n",
      "Resource group: AKS-HCI\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace,  ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download cifar10 data\n",
    "\n",
    "Use this function to download cifar10 data later. This function allows you to avoid download the data again when you run this notebook multiple times. The actual download time may take 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "def download_cifar10_data():\n",
    "    \n",
    "    path = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    downloaded_folder = os.path.join(os.getcwd(), 'cifar10-data')\n",
    "    os.makedirs(downloaded_folder, exist_ok=True) # download data to 'cifar10-data' folder\n",
    "\n",
    "    data = requests.get(path, allow_redirects=True).content\n",
    "    with open(os.path.join(downloaded_folder, path.split('/')[-1]), 'wb') as f:\n",
    "        f.write(data)\n",
    "        \n",
    "    return downloaded_folder\n",
    "\n",
    "downloaded_folder = download_cifar10_data()\n",
    "downloaded_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the cifar10 data to NFS server\n",
    "\n",
    "The following cell will upload \"cifar-10-python.tar.gz\" to NFS server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_mount_path = \"/home/jiadu/data\"\n",
    "\n",
    "import os, shutil\n",
    "cifar10_dir = os.path.join(nfs_mount_path, 'cifar10')\n",
    "shutil.rmtree(cifar10_dir, ignore_errors=True)\n",
    "os.makedirs(cifar10_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(downloaded_folder):\n",
    "    filepath = os.path.join(downloaded_folder, filename)\n",
    "    destpath = os.path.join(cifar10_dir, filename)\n",
    "    print(f\"Copying files from {filepath} to {destpath}\")\n",
    "    shutil.copyfile(filepath, destpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup compute target\n",
    "\n",
    "Find the attach name for the Arc enabled  Azure Stack Hub kubernetes cluster in your AzureML workspace to create a ComputeTarget:\n",
    "\n",
    "attach_name is the attached name for your ASH cluster you setup in [this step](https://github.com/Azure/AML-Kubernetes/blob/master/docs/ASH/AML-ARC-Compute.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import KubernetesCompute\n",
    "\n",
    "attach_name = \"<NAME_OF_AML_ATTACHED_COMPUTE_OF_YOUR_ASH_CLUSTER>\"\n",
    "arcK_target = KubernetesCompute(ws, attach_name)\n",
    "print(\"using compute target: \", arcK_target.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the training job and submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-cifar-distr'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "pytorch_env = Environment.from_conda_specification(name = 'pytorch-1.6-cpu', file_path = 'pytorch-script/conda_dependencies.yml')\n",
    "\n",
    "# Specify a CPU base image\n",
    "pytorch_env.docker.enabled = True\n",
    "pytorch_env.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job: torch.distributed with GLOO backend\n",
    "\n",
    "Create a ScriptRunConfig object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on.\n",
    "\n",
    "In order to run a distributed PyTorch job with **torch.distributed** using the GLOO backend, create a `PyTorchConfiguration` and pass it to the `distributed_job_config` parameter of the ScriptRunConfig constructor. Specify `communication_backend='Gloo'` in the PyTorchConfiguration. The below code will configure node_count = 2. These is the number of worker nodes. The number of  distributed jobs will be 3 if one master node is used.  GLOO backend which is recommended backend for communications between CPUs.\n",
    "\n",
    "Tthe script for distributed training of CIFAR10 is already provided for you at `pytorch-script/cifar_dist_main.py`. In practice, you should be able to take any custom PyTorch training script as is and run it with Azure ML without having to modify your code.\n",
    "\n",
    "With node_count=2, training for one epoch may take 20 mins with vm size comparable to Standard_DS3_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.runconfig import PyTorchConfiguration\n",
    "from azureml.core import Dataset\n",
    "import os\n",
    "\n",
    "data_folder = os.path.join(\"/nfs_share\", \"cifar10\") # training data are saved to <mountPoint>/cifar10\n",
    "args = [\n",
    "        '--data-folder', data_folder,\n",
    "        '--dist-backend', 'gloo',\n",
    "       '--epochs', 1 #20\n",
    "           ]\n",
    "\n",
    "distributed_job_config=PyTorchConfiguration(communication_backend='Gloo', node_count=2) #configuring AML pytorch config\n",
    "\n",
    "project_folder = \"pytorch-script\"\n",
    "run_script = \"cifar_dist_main.py\"\n",
    "src = ScriptRunConfig(\n",
    "                     source_directory=project_folder,\n",
    "                      script=run_script,\n",
    "                      arguments=args,\n",
    "                      compute_target=arcK_target,\n",
    "                      environment=pytorch_env,\n",
    "                      distributed_job_config=distributed_job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job\n",
    "Run your experiment by submitting your ScriptRunConfig object. Note that this call is asynchronous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)\n",
    "run.wait_for_completion(show_output=True) # this provides a verbose log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model\n",
    "\n",
    "Register the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_model_name = 'cifar10torch'\n",
    "model = run.register_model(model_name=register_model_name, model_path='outputs/cifar10torch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model named \"cifar10torch\" should be registered in your AzureML workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model(ws, model_name)\n",
    "model_id = f\"azureml:{model.name}:{model.version}\"\n",
    "print(f\"Get {model.name}, latest version {model.version}, id in endpoint.yml: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and score a machine learning model by using a managed online endpoint\n",
    "\n",
    "AZ CLI only now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'pytorch-cifar10-jiadu'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "prefix = Path(os.getcwd())\n",
    "endpoint_file = str(prefix.joinpath(\"pytorch-cifar10.yml\"))\n",
    "print(f\"Using Endpoint file: {endpoint_file}, please replace model id (<model-id>) and compute target id (<compute-id>) according above output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')\n",
    "helpers.run(f\"az ml endpoint create -n {endpoint} -f {endpoint_file} -w {ws.name} -g {ws.resource_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get score_uri and access_token from AZ CLI (Currently only AZ CLI supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score_url and access_token from AZ CLI\n",
    "import helpers\n",
    "from azureml.core.workspace import Workspace\n",
    "ws = Workspace.from_config()\n",
    "cmd = f\"az ml endpoint show -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "properties = helpers.run(cmd, return_output=True, no_output=True)\n",
    "\n",
    "cmd = f\"az ml endpoint get-credentials -n {endpoint} -w {ws.name} -g {ws.resource_group}\"\n",
    "credentials = helpers.run(cmd, return_output=True, no_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with inputs\n",
    "\n",
    "For testing purpose, first image (a cat) from test batch is extracted (it is saved at test_imgs/test_img_0_cat.jpg). It is shown here: \n",
    "\n",
    "![fishy](test_imgs/test_img_0_cat.jpg)\n",
    "\n",
    "After some data process, the image converted to json as input for the trained model. The outputs are logits for each class per image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prop_response = json.loads(properties.replace(os.linesep,\"\"))\n",
    "score_uri = prop_response[\"scoring_uri\"]\n",
    "\n",
    "cred_response = json.loads(credentials.replace(os.linesep, \"\"))\n",
    "access_token = cred_response[\"accessToken\"]\n",
    "\n",
    "with open(\"cifar_test_input_pytorch.json\", \"r\") as fp:\n",
    "    inputs_json = json.load(fp)\n",
    "    \n",
    "inputs = json.dumps(inputs_json)\n",
    "\n",
    "import requests\n",
    "# second number should be 2\n",
    "test = inputs\n",
    "headers = {'Content-Type': 'application/json', 'Authorization': f\"Bearer {access_token}\"}\n",
    "r = requests.post(score_uri, data=test, headers=headers)\n",
    "print(f\"predictions: {r.content}\")\n",
    "predicts = r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can easily get the predictions of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "np_predicts = np.array(predicts)\n",
    "pred_indexes = np.argmax(np_predicts, 1)\n",
    "\n",
    "predict_labels = [classes[i] for i in pred_indexes]\n",
    "print(predict_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your model, you may or may not get the correct label which is \"cat\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "1. Learn how to [download model then upload to Azure Storage blobs](../AML-model-download-upload.ipynb)\n",
    "2. Learn how to [inference using KFServing with model in Azure Storage Blobs](https://aka.ms/kfas)\n",
    "3. Learn Pipeline Steps with [Object Segmentation](../object-segmentation-on-azure-stack/)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "ninhu"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Distributed training with PyTorch",
  "index_order": 1,
  "interpreter": {
   "hash": "fc402497f0168b24575e2ffafe64cd34c507b9a7fab971a93b09782ae565c5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "tags": [
   "None"
  ],
  "task": "Train a model using distributed training via Nccl/Gloo"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
